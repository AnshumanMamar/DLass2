{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration cell\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "activationFunctions = {\n",
    "    'conv1':'ReLU',\n",
    "    'conv2':'ReLU',\n",
    "    'conv3':'ReLU',\n",
    "    'conv4':'ReLU',\n",
    "    'conv5':'ReLU',\n",
    "    'fc1':'ReLU'\n",
    "}\n",
    "\n",
    "list_kernelSize= [3,3,3,3,3]\n",
    "kernelNumber = [32,32,64,64,128]\n",
    "listDropout = [0,0,0.5]\n",
    "nodesfc1 = 1024\n",
    "classes = 10\n",
    "learningRate = 0.0001\n",
    "lr_schedule = 1 # per 10 epochs half the learningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import RandomCrop, RandomResizedCrop, RandomHorizontalFlip, Resize, CenterCrop, ToTensor, Normalize, Compose\n",
    "# from util import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## util.py##############################################################\n",
    "\n",
    "\n",
    "## dataloader\n",
    "def loader(t1data,valdata,t2data,batch):\n",
    "    bs=batch\n",
    "    bool=True\n",
    "    bool2=False\n",
    "    allLoaders = {\n",
    "        'train' : torch.utils.data.DataLoader(t1data, batch_size=bs,num_workers = 4, shuffle=bool) ,\n",
    "        'valid' : torch.utils.data.DataLoader(valdata, batch_size=bs,num_workers = 4,  shuffle=bool) ,\n",
    "        'test'  : torch.utils.data.DataLoader(t2data, batch_size=bs,num_workers = 4,  shuffle=bool2)\n",
    "    }\n",
    "    return allLoaders\n",
    "\n",
    "\n",
    "## transforms to match realModel input dims\n",
    "def transform():\n",
    "    \n",
    "    valResize = 256 #134 #36\n",
    "    sizeChange = 224 #128#32\n",
    "    valCenterCrop = sizeChange\n",
    "    \n",
    "    #t1_t = Compose([RandomResizedCrop(sizeChange),\n",
    "    string ='Normalize'\n",
    "    t1_t = Compose([RandomResizedCrop(sizeChange),\n",
    "                       RandomHorizontalFlip(),\n",
    "                       ToTensor(),\n",
    "                       Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "    val_t = Compose([Resize(valResize),\n",
    "                       CenterCrop(valCenterCrop),\n",
    "                       ToTensor(),\n",
    "                       Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "    t2_t = Compose([Resize((sizeChange,sizeChange)), \n",
    "                      ToTensor(), \n",
    "                      Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    transforms = {\n",
    "        'training':   t1_t,\n",
    "        'validation': val_t,\n",
    "        'test': t2_t\n",
    "    }\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "## Load dataset fn\n",
    "def data_load():\n",
    "    transforms=transform()\n",
    "    t1set  = torchvision.datasets.ImageFolder('/kaggle/input/dlasssssss2/inaturalist_12K/train', transforms['training'])\n",
    "    train, val = random_split(t1set, [8000, 1999])\n",
    "    t2set   = torchvision.datasets.ImageFolder('/kaggle/input/dlasssssss2/inaturalist_12K/val', transforms['test'])\n",
    "    return train, val, t2set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda == True:\n",
    "    device = torch.device(\"cuda\")\n",
    "if cuda != True:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelName = 'Best_CNN_5Layers_iNaturalist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationFun(activation):\n",
    "    act=activation\n",
    "    if activation == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'GeLU':\n",
    "        return nn.GELU()\n",
    "    elif activation == 'ELU':\n",
    "        return nn.ELU()\n",
    "    elif activation == 'SiLU':\n",
    "        return nn.SiLU()\n",
    "    elif activation == 'Mish':\n",
    "        return nn.Mish()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blockConv(nn.Module):\n",
    "    def __init__(self,channelsIn,channelsOut,kernel= 3 , BN=True , NL=\"relu\",stride = 1, padding = 0):\n",
    "        KL=channelsOut\n",
    "        super(blockConv, self).__init__()\n",
    "        self.BN,self.NL=BN,NL\n",
    "        k = kernel\n",
    "        bol=False\n",
    "        self.conv = nn.Conv2d(channelsIn, channelsOut, kernel_size= k, stride = stride, padding = padding, bias=bol)\n",
    "        bol=self.BN==True\n",
    "        if bol:\n",
    "            val=0.001\n",
    "            self.bn = nn.BatchNorm2d(channelsOut, eps=val)\n",
    "        self.act = activationFun(NL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        bol=self.BN==True\n",
    "        if bol:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_block(nn.Module):\n",
    "    def __init__( self , channelsIn ,channelsOut , BN=False , NL=\"relu\"):\n",
    "        x=channelsOut\n",
    "        super(fc_block, self).__init__()\n",
    "        self.fc = nn.Linear(channelsIn, channelsOut)\n",
    "        self.BN,self.NL=BN,NL\n",
    "       \n",
    "        bol=self.BN==True\n",
    "        if bol:\n",
    "            value=0.001\n",
    "            self.bn = nn.BatchNorm2d(channelsOut, eps=value)    \n",
    "        self.act = activationFun(NL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        bol = self.BN==True\n",
    "        if bol:\n",
    "            value=0.001\n",
    "            x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_in(dim, list_kernelSize, kernelNumber):\n",
    "    fc_in = dim - list_kernelSize[0] + 1 # conv1\n",
    "    val=(fc_in - 2) //2  + 1\n",
    "    fc_in = val # max pool 1\n",
    "\n",
    "    s=1\n",
    "    while s < 5:\n",
    "        fc_in = fc_in - list_kernelSize[s] + 1 # conv2\n",
    "        val1=(fc_in - 2) //2  + 1 \n",
    "        fc_in =val1# max pool \n",
    "        s=s+1 \n",
    "    #print(fc_in)\n",
    "    val2=fc_in * fc_in\n",
    "    ans= val2 * kernelNumber[4]\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_5layer(nn.Module):\n",
    "    def __init__( self,list_kernelSize , kernelNumber , activationFunctions , listDropout, nodesfc1, classes):\n",
    "        list1=list_kernelSize\n",
    "        super(CNN_5layer, self).__init__()\n",
    "        self.listDropout = listDropout\n",
    "        bol2=False\n",
    "        self.dim = 224\n",
    "        self.conv1 = nn.Sequential(blockConv(3 , kernelNumber[0], kernel=list_kernelSize[0], BN=bol2, NL=activationFunctions['conv1']),nn.MaxPool2d((2, 2)))\n",
    "        bol1=True\n",
    "        \n",
    "\n",
    "        self.conv2 = nn.Sequential(blockConv(kernelNumber[0], kernelNumber[1], kernel=list_kernelSize[1], BN=bol1, NL=activationFunctions['conv2']),nn.MaxPool2d((2, 2)))\n",
    "        bol=self.listDropout[0]!=0\n",
    "        if bol:\n",
    "            self.dropout1 = nn.Dropout(listDropout[0])\n",
    "\n",
    "        self.conv3 = nn.Sequential(blockConv(kernelNumber[1], kernelNumber[2], kernel=list_kernelSize[2], BN=bol1, NL=activationFunctions['conv3']),nn.MaxPool2d((2, 2)))\n",
    "        self.conv4 = nn.Sequential(blockConv(kernelNumber[2], kernelNumber[3], kernel=list_kernelSize[3], BN=bol1, NL=activationFunctions['conv4']),nn.MaxPool2d((2, 2)))\n",
    "        bol=self.listDropout[1]!=0\n",
    "        if bol:\n",
    "            self.dropout2 = nn.Dropout(listDropout[1])\n",
    "\n",
    "        self.conv5 = nn.Sequential(blockConv(kernelNumber[3], kernelNumber[4], kernel=list_kernelSize[4], BN=bol1, NL=activationFunctions['conv5']),nn.MaxPool2d((2, 2)))\n",
    "        \n",
    "        self.fc1_in_features = get_fc_in(self.dim, list_kernelSize, kernelNumber)\n",
    "        \n",
    "        self.fc1 = fc_block(self.fc1_in_features, nodesfc1 , NL=activationFunctions['fc1'])\n",
    "        bol=self.listDropout[2]!=0\n",
    "        if bol:\n",
    "            self.dropout3 = nn.Dropout(listDropout[2])\n",
    "        \n",
    "        self.fc2 = nn.Linear(nodesfc1, classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        bol=x.shape[2]!=self.dim\n",
    "        if bol:\n",
    "            print(\"input dim not matched\")\n",
    "            return\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        bol=self.listDropout[0]!=0\n",
    "        if bol:\n",
    "            x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        bol=self.listDropout[1]!=0\n",
    "        if bol:\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "       \n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        bol=self.listDropout[2]!=0\n",
    "        if bol:\n",
    "            x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realModel = CNN_5layer(list_kernelSize, kernelNumber, activationFunctions, listDropout, nodesfc1, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realModel = realModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol=False\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(realModel.parameters(), lr=learningRate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=bol)\n",
    "scheduler = StepLR(opt, step_size=10, gamma=lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(totalEpoch, allLoaders, realModel, opt, criterion,scheduler, cuda):\n",
    "    \n",
    "    for epoch in range(totalEpoch):\n",
    "        \n",
    "        train_loss ,valid_loss= 0.0,0.0\n",
    "        \n",
    "        \n",
    "        ###################\n",
    "        # train the realModel #\n",
    "        ###################\n",
    "        realModel.train()\n",
    "        tnum_correct,tnum_examples=0,0\n",
    "        for data, target in allLoaders['train']:\n",
    "            # move to GPU\n",
    "            bol=cuda\n",
    "            if bol:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            output = realModel(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            tnum_examples += target.size(0)\n",
    "            tnum_correct += (predicted == target).sum().item()\n",
    "            \n",
    "        train_acc = (tnum_correct / tnum_examples) * 100\n",
    "        train_loss = train_loss / len(allLoaders['train'])\n",
    "\n",
    "        \n",
    "        ######################    \n",
    "        # validate the realModel #\n",
    "        ######################\n",
    "        realModel.eval()\n",
    "        num_correct ,num_examples= 0,0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for data, target in allLoaders['valid']:\n",
    "            bol=cuda\n",
    "            if bol:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = realModel(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            \n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            _, val_predicted = torch.max(output.data, 1)\n",
    "            num_examples += target.size(0)\n",
    "            num_correct += (val_predicted == target).sum().item()\n",
    "           \n",
    "\n",
    "        valid_acc = (num_correct / num_examples) * 100\n",
    "        valid_loss = valid_loss / len(allLoaders['valid'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print('Epoch: {}\\tTraining Loss: {:.6f}\\tTrain Accuracy: {:.2f}\\tValidation Loss: {:.6f}\\tvalidation Accuracy: {:.2f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            train_acc,\n",
    "            valid_loss,\n",
    "            valid_acc\n",
    "            ))\n",
    "        \n",
    "        # wandb.log({'epoch': epoch,'train loss': train_loss,'train accuracy': train_acc,\n",
    "        #            'val loss': valid_loss, 'val accuracy': valid_acc})\n",
    "        \n",
    "   \n",
    "    return realModel, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model,_ = train(\n",
    "                      totalEpoch = epochs,\n",
    "                      allLoaders = allLoaders,\n",
    "                      realModel = realModel,\n",
    "                      opt = opt,\n",
    "                      criterion = criterion,\n",
    "                      scheduler = scheduler,\n",
    "                      cuda = cuda\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "trained_model.eval()\n",
    "test_acc,num_correct,num_examples, test_loss = 0,0,0,0\n",
    "loader=allLoaders['test']\n",
    "for data, target in loader:\n",
    "    bol=cuda\n",
    "    if bol:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "    output = trained_model(data)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "\n",
    "\n",
    "    test_loss += loss.item()\n",
    "\n",
    "    _, test_predicted = torch.max(output.data, 1)\n",
    "    num_examples += target.size(0)\n",
    "    num_correct += (test_predicted == target).sum().item()\n",
    "\n",
    "\n",
    "    test_acc = (num_correct / num_examples) * 100\n",
    "    test_loss = test_loss / len(loader)\n",
    "\n",
    "print('Test Accuracy of the realModel is : {}%'.format(test_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"Test Accuracy\": test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_str_list_int(s):\n",
    "    l=1\n",
    "    return list(map(int, s[3:].split('-')))\n",
    "\n",
    "def config_str_list_float(s):\n",
    "    l=1\n",
    "    return list(map(float, s.split('-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_train():\n",
    "    runs=1\n",
    "    configdefaults = {\n",
    "        'epochs': 2,\n",
    "        'kernel_size_config':'1) 5-5-3-3-3' ,\n",
    "        'no_kernel_config':'1) 16-16-16-16-16',\n",
    "        'dropout_config':'0-0-0.4',\n",
    "        'fc1_nodes': 32,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "    # starting a wandb run\n",
    "\n",
    "    wandb.init(config=configdefaults)\n",
    "    start =0\n",
    "    config = wandb.config\n",
    "    epachs=2\n",
    "    sizes= config.kernel_size_config\n",
    "    numbers=config.no_kernel_config\n",
    "    dropout=config.dropout_config\n",
    "    str1=str(config.fc1_nodes)\n",
    "    batchsize=str(config.batch_size)\n",
    "    run_name=\"kSizes:[\"+sizes+\"] kNumbers:[\"+numbers+\"] dp:[\"+dropout+\"] fc1:[\"+str1+\"] bs:[\"+batchsize+\"]\"\n",
    "    wandb.run.name=run_name\n",
    "    \n",
    "   \n",
    "    model = CNN_5layer(config_str_list_int(config.kernel_size_config),\n",
    "                       config_str_list_int(config.no_kernel_config),\n",
    "                       {\n",
    "        'conv1':'relu',\n",
    "        'conv2':'relu',\n",
    "        'conv3':'relu',\n",
    "        'conv4':'relu',\n",
    "        'conv5':'relu',\n",
    "        'fc1':'relu'\n",
    "        },\n",
    "                       config_str_list_float(config.dropout_config),\n",
    "                       config.fc1_nodes, 10).to(device)\n",
    "\n",
    "    opti = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    trained_model,val_accuracy = train(\n",
    "                      totalEpoch = config.epochs,\n",
    "                      allLoaders = allLoaders,\n",
    "                      realModel = model,\n",
    "                      opt = opti,\n",
    "                      criterion = nn.CrossEntropyLoss(),\n",
    "                      scheduler = StepLR(opti, step_size=10, gamma=0.5),\n",
    "                      cuda = cuda\n",
    "                     )\n",
    "    wandb.log({'val_accuracy':val_accuracy})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
