{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration cell\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "activationFunctions = {\n",
    "    'conv1':'ReLU',\n",
    "    'conv2':'ReLU',\n",
    "    'conv3':'ReLU',\n",
    "    'conv4':'ReLU',\n",
    "    'conv5':'ReLU',\n",
    "    'fc1':'ReLU'\n",
    "}\n",
    "\n",
    "list_kernelSize= [3,3,3,3,3]\n",
    "kernelNumber = [32,32,64,64,128]\n",
    "listDropout = [0,0,0.5]\n",
    "nodesfc1 = 1024\n",
    "classes = 10\n",
    "learningRate = 0.0001\n",
    "lr_schedule = 1 # per 10 epochs half the learningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import RandomCrop, RandomResizedCrop, RandomHorizontalFlip, Resize, CenterCrop, ToTensor, Normalize, Compose\n",
    "# from util import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## util.py##############################################################\n",
    "\n",
    "\n",
    "## dataloader\n",
    "def loader(t1data,valdata,t2data,batch):\n",
    "    bs=batch\n",
    "    bool=True\n",
    "    bool2=False\n",
    "    allLoaders = {\n",
    "        'train' : torch.utils.data.DataLoader(t1data, batch_size=bs,num_workers = 4, shuffle=bool) ,\n",
    "        'valid' : torch.utils.data.DataLoader(valdata, batch_size=bs,num_workers = 4,  shuffle=bool) ,\n",
    "        'test'  : torch.utils.data.DataLoader(t2data, batch_size=bs,num_workers = 4,  shuffle=bool2)\n",
    "    }\n",
    "    return allLoaders\n",
    "\n",
    "\n",
    "## transforms to match realModel input dims\n",
    "def transform():\n",
    "    \n",
    "    valResize = 256 #134 #36\n",
    "    sizeChange = 224 #128#32\n",
    "    valCenterCrop = sizeChange\n",
    "    \n",
    "    #t1_t = Compose([RandomResizedCrop(sizeChange),\n",
    "    string ='Normalize'\n",
    "    t1_t = Compose([RandomResizedCrop(sizeChange),\n",
    "                       RandomHorizontalFlip(),\n",
    "                       ToTensor(),\n",
    "                       Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "    val_t = Compose([Resize(valResize),\n",
    "                       CenterCrop(valCenterCrop),\n",
    "                       ToTensor(),\n",
    "                       Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "    t2_t = Compose([Resize((sizeChange,sizeChange)), \n",
    "                      ToTensor(), \n",
    "                      Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    transforms = {\n",
    "        'training':   t1_t,\n",
    "        'validation': val_t,\n",
    "        'test': t2_t\n",
    "    }\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "## Load dataset fn\n",
    "def data_load():\n",
    "    transforms=transform()\n",
    "    t1set  = torchvision.datasets.ImageFolder('/kaggle/input/dlasssssss2/inaturalist_12K/train', transforms['training'])\n",
    "    train, val = random_split(t1set, [8000, 1999])\n",
    "    t2set   = torchvision.datasets.ImageFolder('/kaggle/input/dlasssssss2/inaturalist_12K/val', transforms['test'])\n",
    "    return train, val, t2set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda == True:\n",
    "    device = torch.device(\"cuda\")\n",
    "if cuda != True:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelName = 'Best_CNN_5Layers_iNaturalist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationFun(activation):\n",
    "    act=activation\n",
    "    if activation == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'GeLU':\n",
    "        return nn.GELU()\n",
    "    elif activation == 'ELU':\n",
    "        return nn.ELU()\n",
    "    elif activation == 'SiLU':\n",
    "        return nn.SiLU()\n",
    "    elif activation == 'Mish':\n",
    "        return nn.Mish()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blockConv(nn.Module):\n",
    "    def __init__(self,channelsIn,channelsOut,kernel= 3 , BN=True , NL=\"relu\",stride = 1, padding = 0):\n",
    "        KL=channelsOut\n",
    "        super(blockConv, self).__init__()\n",
    "        self.BN,self.NL=BN,NL\n",
    "        k = kernel\n",
    "        bol=False\n",
    "        self.conv = nn.Conv2d(channelsIn, channelsOut, kernel_size= k, stride = stride, padding = padding, bias=bol)\n",
    "        bol=self.BN==True\n",
    "        if bol:\n",
    "            val=0.001\n",
    "            self.bn = nn.BatchNorm2d(channelsOut, eps=val)\n",
    "        self.act = activationFun(NL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        bol=self.BN==True\n",
    "        if bol:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_block(nn.Module):\n",
    "    def __init__( self , channelsIn ,channelsOut , BN=False , NL=\"relu\"):\n",
    "        x=channelsOut\n",
    "        super(fc_block, self).__init__()\n",
    "        self.fc = nn.Linear(channelsIn, channelsOut)\n",
    "        self.BN,self.NL=BN,NL\n",
    "       \n",
    "        bol=self.BN==True\n",
    "        if bol:\n",
    "            value=0.001\n",
    "            self.bn = nn.BatchNorm2d(channelsOut, eps=value)    \n",
    "        self.act = activationFun(NL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        bol = self.BN==True\n",
    "        if bol:\n",
    "            value=0.001\n",
    "            x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_in(dim, list_kernelSize, kernelNumber):\n",
    "    fc_in = dim - list_kernelSize[0] + 1 # conv1\n",
    "    val=(fc_in - 2) //2  + 1\n",
    "    fc_in = val # max pool 1\n",
    "\n",
    "    s=1\n",
    "    while s < 5:\n",
    "        fc_in = fc_in - list_kernelSize[s] + 1 # conv2\n",
    "        val1=(fc_in - 2) //2  + 1 \n",
    "        fc_in =val1# max pool \n",
    "        s=s+1 \n",
    "    #print(fc_in)\n",
    "    val2=fc_in * fc_in\n",
    "    ans= val2 * kernelNumber[4]\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_5layer(nn.Module):\n",
    "    def __init__( self,list_kernelSize , kernelNumber , activationFunctions , listDropout, nodesfc1, classes):\n",
    "        list1=list_kernelSize\n",
    "        super(CNN_5layer, self).__init__()\n",
    "        self.listDropout = listDropout\n",
    "        bol2=False\n",
    "        self.dim = 224\n",
    "        self.conv1 = nn.Sequential(blockConv(3 , kernelNumber[0], kernel=list_kernelSize[0], BN=bol2, NL=activationFunctions['conv1']),nn.MaxPool2d((2, 2)))\n",
    "        bol1=True\n",
    "        \n",
    "\n",
    "        self.conv2 = nn.Sequential(blockConv(kernelNumber[0], kernelNumber[1], kernel=list_kernelSize[1], BN=bol1, NL=activationFunctions['conv2']),nn.MaxPool2d((2, 2)))\n",
    "        bol=self.listDropout[0]!=0\n",
    "        if bol:\n",
    "            self.dropout1 = nn.Dropout(listDropout[0])\n",
    "\n",
    "        self.conv3 = nn.Sequential(blockConv(kernelNumber[1], kernelNumber[2], kernel=list_kernelSize[2], BN=bol1, NL=activationFunctions['conv3']),nn.MaxPool2d((2, 2)))\n",
    "        self.conv4 = nn.Sequential(blockConv(kernelNumber[2], kernelNumber[3], kernel=list_kernelSize[3], BN=bol1, NL=activationFunctions['conv4']),nn.MaxPool2d((2, 2)))\n",
    "        bol=self.listDropout[1]!=0\n",
    "        if bol:\n",
    "            self.dropout2 = nn.Dropout(listDropout[1])\n",
    "\n",
    "        self.conv5 = nn.Sequential(blockConv(kernelNumber[3], kernelNumber[4], kernel=list_kernelSize[4], BN=bol1, NL=activationFunctions['conv5']),nn.MaxPool2d((2, 2)))\n",
    "        \n",
    "        self.fc1_in_features = get_fc_in(self.dim, list_kernelSize, kernelNumber)\n",
    "        \n",
    "        self.fc1 = fc_block(self.fc1_in_features, nodesfc1 , NL=activationFunctions['fc1'])\n",
    "        bol=self.listDropout[2]!=0\n",
    "        if bol:\n",
    "            self.dropout3 = nn.Dropout(listDropout[2])\n",
    "        \n",
    "        self.fc2 = nn.Linear(nodesfc1, classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        bol=x.shape[2]!=self.dim\n",
    "        if bol:\n",
    "            print(\"input dim not matched\")\n",
    "            return\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        bol=self.listDropout[0]!=0\n",
    "        if bol:\n",
    "            x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        bol=self.listDropout[1]!=0\n",
    "        if bol:\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "       \n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        bol=self.listDropout[2]!=0\n",
    "        if bol:\n",
    "            x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
